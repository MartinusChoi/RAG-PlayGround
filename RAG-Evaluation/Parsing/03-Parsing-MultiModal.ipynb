{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ad6091",
   "metadata": {},
   "source": [
    "# Multimoal Parsing with LlamaParse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed36cf",
   "metadata": {},
   "source": [
    "## 0. Import Library & Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51556e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "from autorag.parser import Parser\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d031e222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff6a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e437dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/martinus/workspace/RAG-Evaluation/Parsing/data/parse_multimodal/Llama_image_text.pdf',\n",
       " '/home/martinus/workspace/RAG-Evaluation/Parsing/data/parse_multimodal/Llama_image.pdf']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "root_dir = cur_dir\n",
    "data_dir = os.path.join(root_dir, \"data\", \"parse_multimodal\")\n",
    "glob_path = os.path.join(data_dir, \"*\")\n",
    "\n",
    "file_path = glob(glob_path)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89113a45",
   "metadata": {},
   "source": [
    "## 1. Parsing with Llama Parse\n",
    "\n",
    "model list\n",
    "- openai-gpt4o\n",
    "- openai-gpt-4o-mini\n",
    "- anthropic-sonnet-3.5\n",
    "- gemini-1.5-flash\n",
    "- gemini-1.5-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ea71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_instance = LlamaParse(\n",
    "    results_type=\"markdown\",\n",
    "    language=\"ko\",\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model=\"openai-gpt-4o-mini\"\n",
    "    #vendor_multimodal_api_key=\"...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb968f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[04/27/25 21:26:05]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=495931;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=373497;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mPOST\u001b[0m                                \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/upload\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m   \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=699988;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=681326;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mPOST\u001b[0m                                \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/upload\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m   \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:26:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=225036;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=225066;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/e36d83f7-b7da-4e6d-b63\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m8-819f3138972d\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:26:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=601879;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=351000;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/6be58682-92bd-4c95-908\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94me-1b044d8f54c2\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:26:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=980107;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=837928;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/e36d83f7-b7da-4e6d-b63\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m8-819f3138972d\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=491545;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=425728;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/6be58682-92bd-4c95-908\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94me-1b044d8f54c2\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=61111;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=662596;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/e36d83f7-b7da-4e6d-b63\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m8-819f3138972d/result/text\u001b[0m          \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                   \u001b[2m               \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files:  50%|█████     | 1/2 [00:06<00:06,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[04/27/25 21:26:12]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=383765;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=599670;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/6be58682-92bd-4c95-908\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94me-1b044d8f54c2\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:26:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=30441;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=180310;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/6be58682-92bd-4c95-908\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94me-1b044d8f54c2\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:26:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=756918;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=123158;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/6be58682-92bd-4c95-908\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94me-1b044d8f54c2\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:26:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=151838;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=802127;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/6be58682-92bd-4c95-908\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94me-1b044d8f54c2\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:26:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=718509;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=636070;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/6be58682-92bd-4c95-908\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94me-1b044d8f54c2\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:26:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=452404;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=882547;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/6be58682-92bd-4c95-908\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94me-1b044d8f54c2\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:26:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=81012;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=448105;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/6be58682-92bd-4c95-908\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94me-1b044d8f54c2/result/text\u001b[0m          \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                   \u001b[2m               \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|██████████| 2/2 [00:38<00:00, 19.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='486cffa4-0fe8-4ae2-98a6-b32d6ae31c25', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\nAs our largest model yet, training Llama on 405B on over 16 trillion tokens was a major challenge. To enable training runs at this scale and achieve the results we have in a reasonable amount of time, we significantly optimized our full training stack and pushed our model training to over 16 thousand H100 GPUs, making the 405B the first Llama model trained at this scale.\\n\\n```\\nINPUT\\nText tokens\\n```\\n→\\n```\\nToken embeddings\\n```\\n→\\n```\\nSelf-attention\\n```\\n→\\n```\\nFeedforward network\\n```\\n→\\n...\\n→\\n```\\nSelf-attention\\n```\\n→\\n```\\nFeedforward network\\n```\\n→\\n```\\nOUTPUT\\nText token\\n```\\n\\n*AUTOREGRESSIVE DECODING*\\n\\nTo address this, we made design choices that focus on keeping the model development process scalable and straightforward.\\n\\n- We opted for a standard decoder-only transformer model architecture with minor adaptations rather than a mixture-of-experts model to maximize training stability.\\n- We adopted an iterative post-training procedure, where each round uses supervised fine-tuning and direct preference optimization. This enabled us to create the highest quality synthetic data for each round and improve each capability’s performance.\\n\\nCompared to previous versions of Llama, we improved both the quantity and quality of the data we use for pre- and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data, the development of more rigorous quality assurance, and filtering approaches for post-training data.\\n\\nAs expected per scaling laws for language models, our new flagship model outperforms smaller models trained using the same procedure. We also used the 405B parameter model to improve the post-training quality of our smaller models.\\n\\nTo support large-scale production inference for a model at the scale of the 405B, we quantized our models from 16-bit (BF16) to 8-bit (FP8) numerics, effectively lowering the compute requirements needed and allowing the model to run within a single server node.\\n\\n## Instruction and chat fine-tuning\\n\\nWith Llama 3.1 405B, we strove to improve the helpfulness, quality, and detailed instruction-following capability of the model in response to user instructions while.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='de26b5ed-eecb-4777-b3e4-b0a22b4e5f98', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n## Benchmark Comparison\\n\\n| Category     | Benchmark                  | Llama 3.1 8B | Gemma 2 9B IT | Mistral 7B Instruct | Llama 3.1 70B | Mistral 8x228 Instruct | GPT 3.5 Turbo |\\n| ------------ | -------------------------- | ------------ | ------------- | ------------------- | ------------- | ---------------------- | ------------- |\\n| General      | MMLU (0-shot, CoT)         | 73.0         | 72.3          | 60.5                | 86.0          | 79.9                   | 69.8          |\\n|              | MMLU PRO (5-shot, CoT)     | 48.3         | -             | 36.9                | 66.4          | 56.3                   | 49.2          |\\n|              | IFEval                     | 80.4         | 73.6          | 57.6                | 87.5          | 72.7                   | 69.9          |\\n| Code         | HumanEval (0-shot)         | 72.6         | 54.3          | 40.2                | 80.5          | 75.6                   | 68.0          |\\n|              | MBPP EvalPlus (0-shot)     | 72.8         | 71.7          | 49.5                | 86.0          | 78.6                   | 82.0          |\\n| Math         | GSM8K (0-shot, CoT)        | 84.5         | 76.7          | 53.2                | 95.1          | 88.2                   | 81.6          |\\n|              | MATH (0-shot, CoT)         | 51.9         | 44.3          | 13.0                | 68.0          | 54.1                   | 43.1          |\\n| Reasoning    | ARC Challenge (0-shot)     | 83.4         | 87.6          | 74.2                | 94.8          | 88.7                   | 83.7          |\\n|              | GPOA (0-shot, CoT)         | 32.8         | -             | 28.8                | 46.7          | 33.3                   | 30.8          |\\n| Tool use     | BFCL                       | 76.1         | -             | 60.4                | 84.8          | -                      | 85.9          |\\n|              | Nexus                      | 38.5         | 30.0          | 24.7                | 56.7          | 48.5                   | 37.2          |\\n| Long context | ZeroSCROLLS/QUALITY        | 81.0         | -             | -                   | 90.5          | -                      | -             |\\n|              | InfiniteBench/Inf.MC       | 65.1         | -             | -                   | 78.2          | -                      | -             |\\n|              | NINJ/Multi-needle          | 98.8         | -             | -                   | 97.5          | -                      | -             |\\n| Multilingual | Multilingual MGSM (0-shot) | 68.9         | 53.2          | 29.9                | 86.9          | 71.1                   | 51.4          |\\n\\n## Llama 3.1 405B Human Evaluation\\n\\n| Comparison                           | Win   | Tie   | Loss  |\\n| ------------------------------------ | ----- | ----- | ----- |\\n| Llama 3.1 405B vs GPT-4-0125-Preview | 23.3% | 52.2% | 24.5% |\\n| Llama 3.1 405B vs GPT-4o             | 19.1% | 51.7% | 29.2% |\\n| Llama 3.1 405B vs Claude 3.5 Sonnet  | 24.9% | 50.8% | 24.2% |\\n\\nSource', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_instance.load_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbec352",
   "metadata": {},
   "source": [
    "## 2. Parsing with AutoRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92d95a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pjt_dir = os.path.join(root_dir, \"multimodal\")\n",
    "parser = Parser(data_path_glob=glob_path, project_dir=pjt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54409a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[04/27/25 21:28:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mparser.py:\u001b[1;36m29\u001b[0m\u001b[1m]\u001b[0m >> Parsing Start\u001b[33m...\u001b[0m     \u001b]8;id=650032;file:///home/martinus/llm/lib/python3.10/site-packages/autorag/parser.py\u001b\\\u001b[2mparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=46662;file:///home/martinus/llm/lib/python3.10/site-packages/autorag/parser.py#29\u001b\\\u001b[2m29\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mbase.py:\u001b[1;36m23\u001b[0m\u001b[1m]\u001b[0m >> Running parser -         \u001b]8;id=520455;file:///home/martinus/llm/lib/python3.10/site-packages/autorag/data/parse/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=464332;file:///home/martinus/llm/lib/python3.10/site-packages/autorag/data/parse/base.py#23\u001b\\\u001b[2m23\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         llama_parse module\u001b[33m...\u001b[0m                    \u001b[2m          \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[04/27/25 21:28:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=572049;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=680111;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mPOST\u001b[0m                                \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/upload\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m   \u001b[2m               \u001b[0m\n",
      "Started parsing the file under job_id f40223ff-3d46-44e2-a4cb-0f693b0e1118\n",
      "\u001b[2;36m[04/27/25 21:28:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=309595;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=455829;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mPOST\u001b[0m                                \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/upload\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m   \u001b[2m               \u001b[0m\n",
      "Started parsing the file under job_id b69d8752-f26c-4be7-af9b-fcd09a0489e6\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=356334;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=431644;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/f40223ff-3d46-44e2-a4c\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mb-0f693b0e1118\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:28:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=704427;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=75497;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/b69d8752-f26c-4be7-af9\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mb-fcd09a0489e6\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:29:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=282222;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=838197;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/f40223ff-3d46-44e2-a4c\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mb-0f693b0e1118\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:29:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=408435;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=824728;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/f40223ff-3d46-44e2-a4c\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mb-0f693b0e1118/result/markdown\u001b[0m      \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                   \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=502430;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=360715;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/b69d8752-f26c-4be7-af9\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mb-fcd09a0489e6\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m    \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m[04/27/25 21:29:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m_client.py:\u001b[1;36m1786\u001b[0m\u001b[1m]\u001b[0m >> HTTP Request:  \u001b]8;id=236767;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=353466;file:///home/martinus/llm/lib/python3.10/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;33mGET\u001b[0m                                 \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://api.cloud.llamaindex.ai/api\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94m/parsing/job/b69d8752-f26c-4be7-af9\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mb-fcd09a0489e6/result/markdown\u001b[0m      \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                   \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mparser.py:\u001b[1;36m37\u001b[0m\u001b[1m]\u001b[0m >> Parsing Done!        \u001b]8;id=260005;file:///home/martinus/llm/lib/python3.10/site-packages/autorag/parser.py\u001b\\\u001b[2mparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=99846;file:///home/martinus/llm/lib/python3.10/site-packages/autorag/parser.py#37\u001b\\\u001b[2m37\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "yaml_path = os.path.join(root_dir, \"config\", \"multimodal.yaml\")\n",
    "parser.start_parsing(yaml_path, all_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644c3db",
   "metadata": {},
   "source": [
    "## 3. Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30828900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>path</th>\n",
       "      <th>page</th>\n",
       "      <th>last_modified_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Introducing Llama 3.1: Our most capable mode...</td>\n",
       "      <td>/home/martinus/workspace/RAG-Evaluation/Parsin...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>| Category     | Benchmark                  | ...</td>\n",
       "      <td>/home/martinus/workspace/RAG-Evaluation/Parsin...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  \\\n",
       "0  # Introducing Llama 3.1: Our most capable mode...   \n",
       "1  | Category     | Benchmark                  | ...   \n",
       "\n",
       "                                                path  page  \\\n",
       "0  /home/martinus/workspace/RAG-Evaluation/Parsin...     1   \n",
       "1  /home/martinus/workspace/RAG-Evaluation/Parsin...     1   \n",
       "\n",
       "  last_modified_datetime  \n",
       "0             2025-04-27  \n",
       "1             2025-04-27  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_path = os.path.join(pjt_dir, 'parsed_result.parquet')\n",
    "result = pd.read_parquet(result_path)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "318b559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Introducing Llama 3.1: Our most capable models to date\n",
      "\n",
      "As our largest model yet, training Llama on 405B on over 16 trillion tokens was a major challenge. To enable training runs at this scale and achieve the results we have in a reasonable amount of time, we significantly optimized our full training stack and pushed our model training to over 16 thousand H100 GPUs, making the 405B the first Llama model trained at this scale.\n",
      "\n",
      "## Model Architecture\n",
      "\n",
      "### Training Process\n",
      "\n",
      "- **Input**: Text tokens\n",
      "- **Token embeddings**\n",
      "- **Self-attention**\n",
      "- **Feedforward network**\n",
      "- **Self-attention**\n",
      "- **Feedforward network**\n",
      "- **Output**: Text token\n",
      "- **Autoregressive Decoding**\n",
      "\n",
      "To address this, we made design choices that focus on keeping the model development process scalable and straightforward.\n",
      "\n",
      "- We opted for a standard decoder-only transformer model architecture with minor adaptations rather than a mixture-of-experts model to maximize training stability.\n",
      "- We adopted an iterative post-training procedure, where each round uses supervised fine-tuning and direct preference optimization. This enabled us to create the highest quality synthetic data for each round and improve each capability’s performance.\n",
      "\n",
      "Compared to previous versions of Llama, we improved both the quantity and quality of the data we use for pre- and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data, the development of more rigorous quality assurance, and filtering approaches for post-training data.\n",
      "\n",
      "As expected per scaling laws for language models, our new flagship model outperforms smaller models trained using the same procedure. We also used the 405B parameter model to improve the post-training quality of our smaller models.\n",
      "\n",
      "## Instruction and Chat Fine-tuning\n",
      "\n",
      "With Llama 3.1 405B, we strove to improve the helpfulness, quality, and detailed instruction-following capability of the model in response to user instructions while maintaining performance.\n"
     ]
    }
   ],
   "source": [
    "print(result['texts'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c556d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Category     | Benchmark                  | Llama 3.1 88B      | Gemma 2 9B IT | Mistral 7B Instruct | Llama 3.1 70B | Mixtral 8x228 Instruct | GPT 3.5 Turbo |      |\n",
      "| ------------ | -------------------------- | ------------------ | ------------- | ------------------- | ------------- | ---------------------- | ------------- | ---- |\n",
      "| General      | MMLU (0-shot, CoT)         | 73.0               | 72.3          | 60.5                | 86.0          | 79.9                   | 69.8          |      |\n",
      "|              | MMLU PRO (5-shot, non-CoT) | 48.3               | 36.9          | 36.9                | 66.4          | 56.3                   | 49.2          |      |\n",
      "|              | IFEval                     | 80.4               | 73.6          | 57.6                | 87.5          | 72.7                   | 69.9          |      |\n",
      "|              | Code                       | HumanEval (0-shot) | 72.6          | 54.3                | 40.2          | 80.5                   | 78.6          | 82.0 |\n",
      "| Math         | MBPP EvalPlus (0-shot)     | 72.8               | 71.7          | 49.5                | 86.0          | 78.6                   | 82.0          |      |\n",
      "|              | GSM8K (0-shot, CoT)        | 84.5               | 76.7          | 53.2                | 95.1          | 88.2                   | 81.6          |      |\n",
      "|              | MATH (0-shot, CoT)         | 51.9               | 44.3          | 13.0                | 68.0          | 54.1                   | 43.1          |      |\n",
      "| Reasoning    | ARC Challenge (0-shot)     | 83.4               | 87.6          | 74.2                | 94.8          | 88.7                   | 83.7          |      |\n",
      "|              | GPAO (0-shot, CoT)         | 32.8               | 28.8          | 46.7                | 33.3          | 30.8                   | -             |      |\n",
      "| Tool use     | BFCL                       | 76.1               | 60.4          | 84.8                | -             | 85.9                   | -             |      |\n",
      "| Long context | ZeroSCROLLS/QUALITY        | 81.0               | -             | -                   | 90.5          | -                      | -             |      |\n",
      "| -            | InfiniteBench/Inf/MC       | 98.8               | -             | -                   | 78.2          | -                      | -             |      |\n",
      "| -            | NIN/Multi-needle           | -                  | -             | -                   | 97.5          | -                      | -             |      |\n",
      "| Multilingual | Multilingual MGSM (0-shot) | 68.9               | 53.2          | 29.9                | 86.9          | 71.1                   | 51.4          |      |\n",
      "\n",
      "| Comparison                           | Win   | Tie   | Loss  |\n",
      "| ------------------------------------ | ----- | ----- | ----- |\n",
      "| Llama 3.1 405B vs GPT-4-0125-Preview | 23.3% | 52.2% | 24.5% |\n",
      "| Llama 3.1 405B vs GPT-4o             | 19.1% | 51.7% | 29.2% |\n",
      "| Llama 3.1 405B vs Claude 3.5 Sonnet  | 24.9% | 50.8% | 24.2% |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result['texts'].tolist()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c9dd4",
   "metadata": {},
   "source": [
    "| Category     | Benchmark                  | Llama 3.1 88B      | Gemma 2 9B IT | Mistral 7B Instruct | Llama 3.1 70B | Mixtral 8x228 Instruct | GPT 3.5 Turbo |      |\n",
    "| ------------ | -------------------------- | ------------------ | ------------- | ------------------- | ------------- | ---------------------- | ------------- | ---- |\n",
    "| General      | MMLU (0-shot, CoT)         | 73.0               | 72.3          | 60.5                | 86.0          | 79.9                   | 69.8          |      |\n",
    "|              | MMLU PRO (5-shot, non-CoT) | 48.3               | 36.9          | 36.9                | 66.4          | 56.3                   | 49.2          |      |\n",
    "|              | IFEval                     | 80.4               | 73.6          | 57.6                | 87.5          | 72.7                   | 69.9          |      |\n",
    "|              | Code                       | HumanEval (0-shot) | 72.6          | 54.3                | 40.2          | 80.5                   | 78.6          | 82.0 |\n",
    "| Math         | MBPP EvalPlus (0-shot)     | 72.8               | 71.7          | 49.5                | 86.0          | 78.6                   | 82.0          |      |\n",
    "|              | GSM8K (0-shot, CoT)        | 84.5               | 76.7          | 53.2                | 95.1          | 88.2                   | 81.6          |      |\n",
    "|              | MATH (0-shot, CoT)         | 51.9               | 44.3          | 13.0                | 68.0          | 54.1                   | 43.1          |      |\n",
    "| Reasoning    | ARC Challenge (0-shot)     | 83.4               | 87.6          | 74.2                | 94.8          | 88.7                   | 83.7          |      |\n",
    "|              | GPAO (0-shot, CoT)         | 32.8               | 28.8          | 46.7                | 33.3          | 30.8                   | -             |      |\n",
    "| Tool use     | BFCL                       | 76.1               | 60.4          | 84.8                | -             | 85.9                   | -             |      |\n",
    "| Long context | ZeroSCROLLS/QUALITY        | 81.0               | -             | -                   | 90.5          | -                      | -             |      |\n",
    "| -            | InfiniteBench/Inf/MC       | 98.8               | -             | -                   | 78.2          | -                      | -             |      |\n",
    "| -            | NIN/Multi-needle           | -                  | -             | -                   | 97.5          | -                      | -             |      |\n",
    "| Multilingual | Multilingual MGSM (0-shot) | 68.9               | 53.2          | 29.9                | 86.9          | 71.1                   | 51.4          |      |\n",
    "\n",
    "| Comparison                           | Win   | Tie   | Loss  |\n",
    "| ------------------------------------ | ----- | ----- | ----- |\n",
    "| Llama 3.1 405B vs GPT-4-0125-Preview | 23.3% | 52.2% | 24.5% |\n",
    "| Llama 3.1 405B vs GPT-4o             | 19.1% | 51.7% | 29.2% |\n",
    "| Llama 3.1 405B vs Claude 3.5 Sonnet  | 24.9% | 50.8% | 24.2% |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
